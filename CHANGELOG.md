# Ход выполнения на 19.12.2024

## Описание проблемы

Эксперименты с бейзлайнами не принесли значимых улучшений в работе модели. Кроме того модель имеет склонность к переобучению. Было принято решение полностью переработать бейзлайн и расширить датасет. Как итог, разработали 4 блокнота:
- Make_BestScore_Dataset.ipynb, в котором происходит расширение датасета двумя наборами ru-izard-emotions и ru-goemotions;
- Make_CL_Dataset.ipynb, в котором происходит раширение датасета наборами двумя наборами ru-izard-emotions и ru-goemotions, предварительная обработка;
- ruBert_base_CL_dataset.ipynb, в котором происходит обучение модели на расширенном CL датасете (CL - clipped);
- ruBert_base.ipynb, в котором происходит предобработка данных, создание модели, ее обучение и оценка качества.

## Работа с данными

Для расширения набора данных использовался датасет ru-izard-emotions из библиотеки datasets, опубликованный пользователем djacon на HuggingFaceHub. Датасет был предобработан для приведения к структуре исходных датасетов train.csv и valid.csv, разделен на обучающую и валидационную выборки, затем эти выборки были объединены соответственно с train.csv и valid.csv. Для токенизации использовали BertTokenizer.

Добавили кодирование one-hot encoding для меток эмоций. Выполнили лемматизацию текста. Удалили стоп-слова.

## Изменения модели

Произвели смену модели ai-forever/ruBert-base на ai-forever/ruBERT-large. Модель имеет Dropout-слой и слой классификации на выходе. В оптимизаторе использовали AdamW, определили Scheduler с расчетом числа шагов обучения. В коде обучения модели реализовали механизм ранней остановки и мониторинга метрики F1-score для борьбы с переобучением.

## Оценка модели

После обучения получены метрики Train loss: 0.162016900270105, Valid loss: 0.26827253162457226, Valid F1: 0.664905310295258

## Выводы

Модель перестала переобучаться, улучшились метрики. Получили на Kaggle score 0.59818.

# Ход выполнения на 13.12.2024

## Работа с данными

На текущем этапе выполнения задачи используются предоставленные датасеты с обучающей выборкой (train.csv), валидационной выборкой (valid.csv), тестовой выборкой с неразмеченными данными (test_without_answers.csv).

В бейзлайне предобработка данных включает в себя избавление текста от капитализаци, знаков пунктуации, удаление небуквенных символов и пробелов. Наша команда включила также нормализацию, анализ пропусков и дубликатов с последующим удалением.

В токенизацию добавлено автоматическое определние длины текста.

## Изменения модели

В качестве изменений выполнено:
- динамическое определение размера скрытых слоёв вместо жёсткого указания hidden_dim=768;
- Dropout-слой для регуляризации.

В оптимизаторе используем AdamW вместо Adam и добавляем регуляризацию L2.

При обучении внедрили Scheduler для постепенного уменьшения Learning rate, для чего также рассчитываем ошибку.

## Оценка модели

На валидационных данных при обучении в течение трех эпох получили значение Valid loss: 0.23307764687958885, на неразмеченных данных - Valid loss: 0.2817318362557378.

## Выводы

Получили на Kaggle score 0.54904. Модель имеет склонность к переобучению при количестве эпох больше трех.
